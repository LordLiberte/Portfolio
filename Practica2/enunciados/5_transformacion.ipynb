{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir('./../')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Escalado en estaciones\n",
    "Aplicación de transformaciones de variables para analizar los datos de estaciones.\n",
    "\n",
    "El escalado tiene como objetivo obtener generalizaciones mejores en el modelado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfe = pd.read_csv('./data/interim/estaciones.csv')\n",
    "dfe.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Preprocesamiento con Scikitlearn, escalado lineal, no lineal, tranformaciones, codificación de categorías](https://qu4nt.github.io/sklearn-doc-es/modules/preprocessing.html#preprocessing-data)\n",
    "\n",
    "La normalización lineal uniforme: $v' = \\frac{v - \\min}{\\max - \\min}$ es un ejemplo en el que los datos pasan a estar entre 0 y 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atributo = 'uso_bici'\n",
    "# Ejemplo de normalización\n",
    "dfe[f\"{atributo}_norm\"] = (dfe[atributo] - dfe[atributo].min()) / (dfe[atributo].max() - dfe[atributo].min())\n",
    "# Ejemplo transformaciones\n",
    "dfe[f\"{atributo}_normed\"] = (dfe[atributo] - dfe[atributo].mean())\n",
    "dfe[f\"{atributo}_tip\"] = (dfe[atributo] - dfe[atributo].mean()) / dfe[atributo].std()\n",
    "dfe[f\"{atributo}_sqrt\"] = np.sqrt(dfe[atributo])\n",
    "dfe[f\"{atributo}_pow\"] = np.pow(dfe[atributo], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=(6, 5))\n",
    "axs = ax.flatten()\n",
    "ax = dfe[atributo].hist(bins=20, ax=axs[0])\n",
    "ax.set_title('Original')\n",
    "ax = dfe[f\"{atributo}_norm\"].hist(bins=20, ax=axs[1], color='g')\n",
    "ax.set_title('Normalizado')\n",
    "ax = dfe[f\"{atributo}_normed\"].hist(bins=20, ax=axs[2], color='gray')\n",
    "ax.set_title('Normalizado media cero')\n",
    "ax = dfe[f\"{atributo}_tip\"].hist(bins=20, ax=axs[3], color='y')\n",
    "ax.set_title('Tipificar')\n",
    "ax = dfe[f\"{atributo}_sqrt\"].hist(bins=20, ax=axs[4], color='orange')\n",
    "ax.set_title('Raíz')\n",
    "ax = dfe[f\"{atributo}_pow\"].hist(bins=20, ax=axs[5], color='pink')\n",
    "ax.set_title('Potencia')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisamos las unidades\n",
    "x_feat, y_feat, size = 'lon', 'lat', 'uso_bici'\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 4))\n",
    "ax = ax.scatter(dfe[x_feat], dfe[y_feat], \n",
    "                s=(dfe[size]*0.05), c=dfe[size], cmap='viridis')\n",
    "plt.ylabel(y_feat)\n",
    "plt.xlabel(x_feat)\n",
    "fig.tight_layout()\n",
    "cbar = plt.colorbar(ax)\n",
    "cbar.set_label(size)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_feat, y_feat, size = 'lon', 'lat', 'uso_bici'\n",
    "# Trabajamos con la librería scikit-learn\n",
    "# prep_scaling = preprocessing.MinMaxScaler()\n",
    "# prep_scaling = preprocessing.Normalizer(\"l1\")\n",
    "# prep_scaling = preprocessing.StandardScaler()\n",
    "# prep_scaling = preprocessing.MaxAbsScaler()\n",
    "# prep_scaling = preprocessing.RobustScaler()\n",
    "# prep_scaling = preprocessing.QuantileTransformer(n_quantiles=6)\n",
    "\n",
    "# Primero se entrena/ajusta el transformador\n",
    "transf = prep_scaling.fit(dfe.loc[:, [x_feat, y_feat, size]].values)\n",
    "\n",
    "# Luego se transforman los datos\n",
    "data_transf = transf.transform(dfe.loc[:, [x_feat, y_feat, size]].values)\n",
    "\n",
    "# Creamos el dataframe con el resultado\n",
    "df2gm = pd.DataFrame(data_transf, \n",
    "                     columns=[x_feat, y_feat, size])\n",
    "df2gm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_feat, y_feat, size = 'lon', 'lat', 'uso_bici'\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 4))\n",
    "ax = ax.scatter(df2gm[x_feat], df2gm[y_feat], \n",
    "                s=(df2gm[size]+1)**2, c=df2gm[size],\n",
    "                cmap='viridis')\n",
    "fig.suptitle(\"Transformación por cuantiles - 5\")\n",
    "plt.ylabel(y_feat)\n",
    "plt.xlabel(x_feat)\n",
    "fig.tight_layout()\n",
    "cbar = plt.colorbar(ax)\n",
    "cbar.set_label(size)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_feat, y_feat, size = 'lon', 'lat', 'uso_bici'\n",
    "standard_scaler = preprocessing.StandardScaler()\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# Definir el parámetro de quantiles\n",
    "quantile_scaler = preprocessing.QuantileTransformer(n_quantiles=5)\n",
    "normalizer = preprocessing.Normalizer()\n",
    "dfe[[x_feat, y_feat, size]] = standard_scaler.fit_transform(dfe[[x_feat, y_feat, size]].values)\n",
    "minmax_data = min_max_scaler.fit_transform(dfe[[x_feat, y_feat, size]].values)\n",
    "quantile_data = quantile_scaler.fit_transform(dfe[[x_feat, y_feat, size]].values)\n",
    "normalize_data = normalizer.fit_transform(dfe[[x_feat, y_feat, size]].values)\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "multip = 3\n",
    "# Plot the data on each subplot\n",
    "axs[0, 0].scatter(minmax_data[:,0], minmax_data[:,1],\n",
    "                  label='MinMaxScaler', s=(dfe[size]+1)**multip, \n",
    "                  c=dfe[size], cmap='viridis')\n",
    "axs[0, 0].set_title('MinMaxScaler')\n",
    "axs[0, 0].legend()\n",
    "\n",
    "axs[0, 1].scatter(quantile_data[:,0], quantile_data[:,1], \n",
    "                  label='QuantileTransformer', s=(dfe[size]+1)**multip, \n",
    "                  c=dfe[size], cmap='viridis')\n",
    "axs[0, 1].set_title('QuantileTransformer')\n",
    "axs[0, 1].legend()\n",
    "\n",
    "axs[1, 0].scatter(normalize_data[:,0], normalize_data[:,1], \n",
    "                  label='Normalizer', s=(dfe[size]+1)**multip, \n",
    "                  c=dfe[size], cmap='viridis')\n",
    "axs[1, 0].set_title('Normalizer')\n",
    "axs[1, 0].legend()\n",
    "\n",
    "# Original data in yellow is plotted on the last subplot\n",
    "axs[1, 1].scatter(dfe['lon'], dfe['lat'], label='StandardScaler', \n",
    "                  s=(dfe[size]+1)**multip, c=dfe[size], cmap='viridis')  \n",
    "axs[1, 1].set_title('StandardScaler')\n",
    "axs[1, 1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/processed/usobarriosmeteo.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uso de bicis\n",
    "En el dataset objetivo \"usobarriosmeteo.csv\" vamos a transformar los datos para ampliar características.\n",
    "> Hipótesis: la hora del día puede ser un factor que defina el comportamiento del cliente según la zona de la ciudad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/interim/usobarriosmeteo.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar a datetime si no tiene el tipo \n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "df['hora'] = pd.to_datetime(df['fecha']).dt.hour\n",
    "df['dia'] = pd.to_datetime(df['fecha']).dt.day\n",
    "df['mes'] = pd.to_datetime(df['fecha']).dt.month\n",
    "df['anio'] = pd.to_datetime(df['fecha']).dt.year\n",
    "df['dia_nombre'] = pd.to_datetime(df['fecha']).dt.day_name()\n",
    "# Monday=0, Sunday=6\n",
    "df['dia_semana'] = df['fecha'].dt.day_of_week\n",
    "df['findesemana'] = np.where(df['dia_semana']>4, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dia_nombre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/processed/usobarriosmeteo.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones\n",
    "- Se tiene exploran las opciones para transformar datos\n",
    "- El escalado puede ser útil para datos con unidades muy dispares\n",
    "- La extracción de características puede mejorar el aprendizaje automático"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mientorno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
